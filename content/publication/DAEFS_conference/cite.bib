@inproceedings{DAEFS_WCCI2022,,
abstract = {We propose the simple and efficient method of semi-supervised learning for deep neural networks. Basically, the proposed network is trained in a supervised fashion with labeled and unlabeled data simultaneously. For un-labeled data, Pseudo-Labels, just picking up the class which has the maximum predicted probability, are used as if they were true labels. This is in effect equivalent to Entropy Regularization. It favors a low-density separation between classes, a commonly assumed prior for semi-supervised learning. With Denoising Auto-Encoder and Dropout, this simple method outperforms conventional methods for semi-supervised learning with very small labeled data on the MNIST handwritten digit dataset.},
author = {Aritake, Toshimitsu and Hino, Hideitsu},
journal = {2022 International Joint Conference on Neural Networks (IJCNN)},
% pages = {1--6},
title = {Domain Adaptation with Optimal Transport for Extended Variable Space},
year = {2022}
}
